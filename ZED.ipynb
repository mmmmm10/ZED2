{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Angry Tweets\n",
    "\n",
    "Import używanych bibliotek i kompilacja zastosowanych wyrażeń regularnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "RE_SPACES = re.compile(\"\\s+\")\n",
    "RE_HASHTAG = re.compile(\"[@#][_a-z0-9]+\")\n",
    "RE_EMOTICONS = re.compile(\"(:-?\\))|(:p)|(:d+)|(:-?\\()|(:/)|(;-?\\))|(<3)|(=\\))|(\\)-?:)|(:'\\()|(8\\))\")\n",
    "RE_HTTP = re.compile(\"http(s)?://[/\\.a-z0-9]+\")\n",
    "RE_NUM = re.compile(\"[0-9]+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie danych treningowych w celu utworzenia słownika. Wyrzucam wszystkie tweety z tekstem 'Not Available', ponieważ oceniane są różnie i wprowadzają szum. Można próbować zastąpić je kopiując losowy tweet z danej klasy jednak nie wniesie to nowej jakości do danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"train.csv\", sep=\",\", index_col=None, na_values = 'Not Available').iloc[:,2]\n",
    "tweets = tweets.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deklaracja metody obsługującej przetworzenie encji html na poprawne znaki tekstowe oraz deklaracja tokenizatora dla tweetów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BeforeTokenizationNormalizer():\n",
    "    @staticmethod\n",
    "    def normalize(text):\n",
    "        text = text.strip().lower()\n",
    "        text = text.replace('&nbsp;', ' ')\n",
    "        text = text.replace('&lt;', '<')\n",
    "        text = text.replace('&gt;', '>')\n",
    "        text = text.replace('&amp;', '&')\n",
    "        text = text.replace('&pound;', u'£')\n",
    "        text = text.replace('&euro;', u'€')\n",
    "        text = text.replace('&copy;', u'©')\n",
    "        text = text.replace('&reg;', u'®')\n",
    "        return text\n",
    "\n",
    "class TweetTokenizer():\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        tokens = text.split()\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            match = re.search(RE_HASHTAG, token) or re.search(RE_EMOTICONS, token) or re.search(RE_HTTP, token)\n",
    "            if match is not None:\n",
    "                del tokens[i]\n",
    "                i -= 1\n",
    "                match = None\n",
    "            else:\n",
    "                del tokens[i]\n",
    "                tokens[i:i] = nltk.word_tokenize(token)\n",
    "            i += 1\n",
    "            \n",
    "        porter = nltk.PorterStemmer()\n",
    "        tokens2 = []\n",
    "        for token in tokens:\n",
    "            token2 = porter.stem(token)\n",
    "            if token2 == \"n't\":\n",
    "                token2 = \"not\"\n",
    "            if token2 not in string.punctuation:\n",
    "                tokens2.append(token2)\n",
    "        return tokens2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stworzenie słownika dla tweetów pomijającego znaki interpunkcyjne i cyfry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    tweet = BeforeTokenizationNormalizer.normalize(tweets.iat[i])\n",
    "    words.update(TweetTokenizer.tokenize(tweet))\n",
    "\n",
    "words2 = dict(words)\n",
    "for word in words:\n",
    "    for letter in word:\n",
    "        if letter in string.punctuation:\n",
    "            del words2[word]\n",
    "            break\n",
    "\n",
    "words3 = dict(words2)\n",
    "for i in words2:\n",
    "    if re.search(RE_NUM,i):\n",
    "        del words3[i]\n",
    "words = Counter(words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opisanie tweetów w reprezentacji bag of words i zamknięcie ich w macierzy CSR (skompresowana wierszami postać macierzy rzadkiej) w celu zmniejszenia jej rozmiaru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_bow(documents, features):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(documents)):\n",
    "        tweet = BeforeTokenizationNormalizer.normalize(documents.iat[i, 2])\n",
    "        label = documents.iloc[i, 1]\n",
    "        tweet_tokens = TweetTokenizer.tokenize(tweet)\n",
    "\n",
    "        labels.append(label)\n",
    "        for token in set(tweet_tokens):\n",
    "            if token not in features:\n",
    "                continue\n",
    "            row.append(i)\n",
    "            col.append(features[token])\n",
    "            data.append(1)\n",
    "    return csr_matrix((data, (row, col)), shape=(len(documents), len(features))), labels\n",
    "\n",
    "def create_bow2(documents, features):\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    for i in range(len(documents)):\n",
    "        tweet = BeforeTokenizationNormalizer.normalize(documents.iat[i, 1])\n",
    "        tweet_tokens = TweetTokenizer.tokenize(tweet)\n",
    "\n",
    "        for token in set(tweet_tokens):\n",
    "            if token not in features:\n",
    "                continue\n",
    "            row.append(i)\n",
    "            col.append(features[token])\n",
    "            data.append(1)\n",
    "    return csr_matrix((data, (row, col)), shape=(len(documents), len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzenie klasyfikatora i ocena tweetów testowych. Minimalna liczba wystąpień słowa została dobrana eksperymentalnie i wynosi 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_word_count = 50\n",
    "\n",
    "train_tweets = pd.read_csv(\"train.csv\", sep=\",\", na_values = 'Not Available')\n",
    "train_tweets = train_tweets.dropna()\n",
    "common_words = list([k for k, v in words.most_common() if v > min_word_count])\n",
    "\n",
    "feature_dict = {}\n",
    "for word in common_words:\n",
    "    feature_dict[word] = len(feature_dict)\n",
    "\n",
    "X_train, y_train = create_bow(train_tweets, feature_dict)\n",
    "list_of_labels = list(set(y_train))\n",
    "classifier = RandomForestClassifier(criterion = \"entropy\", n_estimators=50, n_jobs=-1, random_state=111)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "test_tweets = pd.read_csv(\"test.csv\", sep=\",\", na_values = 'nan', dtype = str)\n",
    "test_tweets = test_tweets.dropna()\n",
    "X_test = create_bow2(test_tweets, feature_dict)\n",
    "predicted = classifier.predict(X_test)\n",
    "\n",
    "f = open(\"sub.csv\", 'w')\n",
    "f.write(\"Id,Category\\n\")\n",
    "for i in range(len(test_tweets)):\n",
    "    f.write(test_tweets.iat[i,0]+','+predicted[i]+'\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
